{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a991f34b7494a75a4be803ed07deb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cab8b200023340d7acebec2f94c1427a",
              "IPY_MODEL_328ee21afc5a44d0b7c06eba438cd69b",
              "IPY_MODEL_ec8b1712079e431eb97433dec0113027"
            ],
            "layout": "IPY_MODEL_dbffceeb97ca41aa8cbb2c7a0890b6f3"
          }
        },
        "cab8b200023340d7acebec2f94c1427a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6326cc0fc52242ba96800cafb2c7bae5",
            "placeholder": "​",
            "style": "IPY_MODEL_d8e2de0ec7dc45409636d47d0756e5d1",
            "value": "Map:  81%"
          }
        },
        "328ee21afc5a44d0b7c06eba438cd69b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f50161ef985e4968b79817a13179f419",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fc238d9c87b4e238f292372daee382d",
            "value": 100
          }
        },
        "ec8b1712079e431eb97433dec0113027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ae5530b7392437f868716ab26d6050a",
            "placeholder": "​",
            "style": "IPY_MODEL_c51c0ce3c6a143d296c5af83948ee35c",
            "value": " 81/100 [00:00&lt;00:00, 279.37 examples/s]"
          }
        },
        "dbffceeb97ca41aa8cbb2c7a0890b6f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "6326cc0fc52242ba96800cafb2c7bae5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8e2de0ec7dc45409636d47d0756e5d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f50161ef985e4968b79817a13179f419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fc238d9c87b4e238f292372daee382d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ae5530b7392437f868716ab26d6050a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c51c0ce3c6a143d296c5af83948ee35c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAfAlc3ggeDn"
      },
      "outputs": [],
      "source": [
        "# If you are using PyTorch backendpy\n",
        "!pip install torch==2.0.1\n",
        "!pip install transformers @ git+https://github.com/huggingface/transformers@de9255de27abfcae4a1f816b904915f0b1e23cd9\n",
        "#lightning @ git+https://github.com/Lightning-AI/lightning@master\n",
        "!pip install tokenizers==0.13.3\n",
        "!pip install peft\n",
        "!pip install jsonargparse[signatures]  # CLI\n",
        "!pip install bitsandbytes==0.39.1 # quantize\n",
        "!pip install accelerate @ git+https://github.com/huggingface/accelerate@e0f5e030098aada5e112708eee3537475dea3a83\n",
        "!pip install datasets==2.13.1  # quantize/gptq.py\n",
        "!pip install zstandard==0.19.0  # prepare_redpajama.py\n",
        "!pip install scipy\n",
        "!pip install loralib==0.1.1\n",
        "!pip install einops==0.6.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your CSV file in Google Drive\n",
        "file_path = 'train_samples.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "training_data = pd.read_csv(file_path)\n",
        "training_data = training_data[:100]"
      ],
      "metadata": {
        "id": "Iq48qthf4v4h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data.head(3)"
      ],
      "metadata": {
        "id": "YDkY7Lpz5-53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# model_name = \"tiiuae/falcon-7b-instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"tiiuae/falcon-7b-instruct\",\n",
        "    load_in_4bit=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"tiiuae/falcon-7b-instruct\",\n",
        ")"
      ],
      "metadata": {
        "id": "9l8IL_ceguR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UrKpU2Dmv_yS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_config = model.generation_config\n",
        "gen_config.max_new_tokens = 200\n",
        "gen_config.temperature = 0.0\n",
        "gen_config.num_return_sequences = 1\n",
        "gen_config.pad_token_id = tokenizer.eos_token_id\n",
        "gen_config.eos_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "nFpNChd-jw-d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt = training_data.loc[0, 'question']\n",
        "\n",
        "encoding = tokenizer(test_prompt, return_tensors= \"pt\").to(model.device)"
      ],
      "metadata": {
        "id": "pgoiDvqXmQex"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ltPVpnF18enj",
        "outputId": "4644e474-d565-4735-cd49-7cbb87a19a76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Prompt: \"The formulation includes 450.0 kg of GGBFS and Metakaolin powder at a ratio of 90.0/10.0. 1716.0 kg of aggregate, and water-to-powder ratio is 0.33. Powder oxide composition is: SiO2: 34.38, Al2O3: 17.87, Fe2O3: 0.75, CaO: 36.8. The amount of the dry fraction of Na2O from the activator solution is 15.44 kg. The amount of dry fraction in the activator of SiO2 is 30.87 kg, the amount of Na(OH) is 53.0 kg, with a molarity of 14.0. The mix also includes 22.5 kg of superplasticizer. and 54.0 kg of extra water.The compressive strength for a cylindrical sample (100.0 mm/200.0 mm) after 7 days was? answer\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute for testing purposes\n",
        "# import torch\n",
        "\n",
        "# with torch.inference_mode():\n",
        "#    outputs = model.generate(input_ids = encoding.input_ids, attention_mask = encoding.attention_mask,generation_config = gen_config )\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
      ],
      "metadata": {
        "id": "13jlzHclguUv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def gen_prompt(text_input):\n",
        "    return f\"\"\"{text_input[\"question\"]}: {text_input[\"answer\"]}\n",
        "    \"\"\".strip()\n",
        "\n",
        "def gen_and_tok_prompt(text_input):\n",
        "    full_input = gen_prompt(text_input)\n",
        "    tok_full_prompt = tokenizer(full_input, padding = True , truncation =True)\n",
        "    return tok_full_prompt\n",
        "\n",
        "# X = {\n",
        "#     'question': ['a', 'c', 'e'],\n",
        "#     'answer': ['b', 'd', 'f']\n",
        "# }\n",
        "\n",
        "df = training_data.copy()\n",
        "\n",
        "data = Dataset.from_pandas(df[['question', 'answer']])"
      ],
      "metadata": {
        "id": "AZfmudwRguYL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "X8N2R7eUsnAv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.map(gen_and_tok_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "0a991f34b7494a75a4be803ed07deb47",
            "cab8b200023340d7acebec2f94c1427a",
            "328ee21afc5a44d0b7c06eba438cd69b",
            "ec8b1712079e431eb97433dec0113027",
            "dbffceeb97ca41aa8cbb2c7a0890b6f3",
            "6326cc0fc52242ba96800cafb2c7bae5",
            "d8e2de0ec7dc45409636d47d0756e5d1",
            "f50161ef985e4968b79817a13179f419",
            "5fc238d9c87b4e238f292372daee382d",
            "2ae5530b7392437f868716ab26d6050a",
            "c51c0ce3c6a143d296c5af83948ee35c"
          ]
        },
        "id": "HBzF-1o3nCOX",
        "outputId": "5731c447-8003-4b9e-b907-5e96c4bbea84"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a991f34b7494a75a4be803ed07deb47"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "XGV3mKJEnGrh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "RdjCQrjSnGuZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "KB7X1ujkv4lb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QewSl0smnUQp",
        "outputId": "122a1b96-bc49-4299-db64-ccc948eff84b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4718592 || all params: 3613463424 || trainable%: 0.13058363808693696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.enable_grad()"
      ],
      "metadata": {
        "id": "0qvHboUC-wcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data"
      ],
      "metadata": {
        "id": "e29DtNc2Bcue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ],
      "metadata": {
        "id": "zjwsKqmM-4UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    save_total_limit=4,\n",
        "    logging_steps=25,\n",
        "    output_dir=\"results\", # give the location where you want to store checkpoints\n",
        "    save_strategy='epoch',\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type = 'cosine',\n",
        "    warmup_ratio = 0.05,\n",
        "    report_to='tensorboard'\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    args=training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")"
      ],
      "metadata": {
        "id": "lbzbuKfn_I5x"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "py9F3g6znUWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('results/concrete_falcon')"
      ],
      "metadata": {
        "id": "3zAAU_Z-s19X"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "_lm3ZC2uuAfa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "config = PeftConfig.from_pretrained('results/concrete_falcon')\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    load_in_4bit=True,\n",
        "#     device_map='auto',\n",
        "    trust_remote_code=True,\n",
        "\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config.base_model_name_or_path)\n",
        "\n",
        "model_inf = PeftModel.from_pretrained(model, 'results/concrete_falcon')\n"
      ],
      "metadata": {
        "id": "H-SzQovJtIdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "prompt = f\"\"\"{test_prompt}: \"\"\".strip()\n",
        "\n",
        "# encode the prompt\n",
        "encoding = tokenizer(prompt, return_tensors= \"pt\").to(model.device)\n",
        "\n",
        "gen_config = model_inf.generation_config\n",
        "gen_config.max_new_tokens = 200\n",
        "gen_config.temperature = 0.0\n",
        "gen_config.num_return_sequences = 1\n",
        "gen_config.pad_token_id = tokenizer.eos_token_id\n",
        "gen_config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# do the inference\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(input_ids = encoding.input_ids,\n",
        "                             attention_mask = encoding.attention_mask,\n",
        "                             generation_config = gen_config )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens = True ))"
      ],
      "metadata": {
        "id": "a57LkpwPtlwU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}