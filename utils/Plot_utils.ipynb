{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d64e23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import math\n",
    "from matplotlib import patches\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import re\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, Button\n",
    "import collections\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_info(filename):\n",
    "    pattern = r\"(?P<model>[^_]+)_(?P<prompt>[^_\\d]+)(?: \\d+)?_prompt_experiment_.*_temp_(?P<temp>.*?)_target_(?P<target>.*?)_%_Dev_Budget_(?P<budget>\\d+)_recursive_(?P<recursive>\\d+)_(\\d+)\\.csv\"\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        return match.group('model'), match.group('prompt'), match.group('temp'), match.group('target'), int(match.group('budget')), int(match.group('recursive'))\n",
    "    return None\n",
    "\n",
    "def unique_combinations(directory):\n",
    "    unique_lists = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            info = extract_info(filename)\n",
    "            if info:\n",
    "                unique_lists.append(info)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Remove duplicates and sort\n",
    "    unique_lists = list(set(unique_lists))\n",
    "    unique_lists.sort(key=lambda x: (float(x[2]), x[0], x[1], x[5]))\n",
    "\n",
    "    \n",
    "    return unique_lists\n",
    "\n",
    "\n",
    "def load_data(filename, budget):\n",
    "    df = pd.read_csv(filename)\n",
    "    strength = df['Compressive Strength'].values\n",
    "    if len(strength) < budget:\n",
    "        last_value = strength[-1] if len(strength) > 0 else 0\n",
    "        strength = np.pad(strength, (0, budget - len(strength)), constant_values=last_value)\n",
    "    return strength\n",
    "\n",
    "def load_data_baseline(filename, budget):\n",
    "    df = pd.read_csv(filename)\n",
    "    strength = df['fc_28dGroundTruth'].values\n",
    "    if len(strength) > 4:  # if more than initial samples, drop them\n",
    "        strength = strength[4:]  # drop first 4 samples\n",
    "    if len(strength) < budget:\n",
    "        last_value = strength[-1] if len(strength) > 0 else 0\n",
    "        strength = np.pad(strength, (0, budget - len(strength)), 'constant', constant_values=last_value)\n",
    "    return strength\n",
    "\n",
    "\n",
    "def load_selected_data(btn):\n",
    "    data = collections.defaultdict(list)\n",
    "\n",
    "    for selected in combo_widget.value:\n",
    "        split_selected = selected.split(\", \")\n",
    "\n",
    "        # For regular models\n",
    "        if len(split_selected) == 5 and \"Prompt\" in selected:  # Make sure there are 5 elements\n",
    "            selected_model = split_selected[0].split(\": \")[1]\n",
    "            selected_prompt = split_selected[1].split(\": \")[1]\n",
    "            selected_temp = split_selected[2].split(\": \")[1]\n",
    "            selected_target = split_selected[3].split(\": \")[1]\n",
    "            selected_chain = re.split(r\":\\s*\", split_selected[4].strip())[1]\n",
    "\n",
    "            for filename in os.listdir('Results/ID'):\n",
    "                info = extract_info(filename)\n",
    "                \n",
    "                if info and info[:-2] == (selected_model, selected_prompt, selected_temp, selected_target) and str(info[-1]) == selected_chain:\n",
    "                    strength = load_data(os.path.join('Results/ID', filename), info[-2])  # Use the budget, which is second to last\n",
    "                    data[(selected_model, selected_prompt, selected_temp, selected_target, selected_chain)].append(strength)\n",
    "\n",
    "        # For baseline models\n",
    "        elif len(split_selected) == 4 and \"Initial Samples\" in selected:\n",
    "            selected_model, selected_init_samples, selected_target, selected_budget = split_selected\n",
    "            selected_model = selected_model.split(\": \")[1]\n",
    "            selected_init_samples = selected_init_samples.split(\": \")[1]\n",
    "            selected_target = selected_target.split(\": \")[1]\n",
    "            selected_budget = selected_budget.split(\": \")[1]\n",
    "            if selected_model == 'BO':\n",
    "                for directory in ['Results/BO']:\n",
    "                    for filename in os.listdir(directory):\n",
    "                        try:\n",
    "                            info = extract_info_baseline(filename)\n",
    "                            strength = load_data_baseline(os.path.join(directory, filename), info[-1])\n",
    "                            data[info[:-1]].append(strength)\n",
    "                        except:\n",
    "                            pass\n",
    "            elif selected_model =='RF':\n",
    "                for directory in ['Results/RF_ID']:\n",
    "                    for filename in os.listdir(directory):\n",
    "                        try:\n",
    "                            info = extract_info_baseline(filename)\n",
    "                            strength = load_data_baseline(os.path.join(directory, filename), info[-1])\n",
    "                            data[info[:-1]].append(strength)\n",
    "                        except:\n",
    "                            pass\n",
    "            elif selected_model =='RP':\n",
    "                for directory in ['Results/RP_ID']:\n",
    "                    for filename in os.listdir(directory):\n",
    "                        try:\n",
    "                            info = extract_info_baseline(filename)\n",
    "                            strength = load_data_baseline(os.path.join(directory, filename), info[-1])\n",
    "                            data[info[:-1]].append(strength)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "    print(\"Data Loaded Successfully!\")\n",
    "    \n",
    "    return data\n",
    "def extract_info_baseline(filename):\n",
    "    pattern = r\"experiment_(?P<experiment>\\d+)_(?P<model>.*?)_(initialsample_(?P<initialsample>\\d+)?_)?target_(?P<target>.*?)_%_Dev_Budget_(?P<budget>\\d+)_.*\"\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        model = match.group('model')\n",
    "        experiment = int(match.group('experiment'))\n",
    "        initial_sample_size = int(match.group('initialsample')) if match.group('initialsample') else 0\n",
    "        target = match.group('target')\n",
    "        budget = int(match.group('budget'))\n",
    "        return model, initial_sample_size, target, budget\n",
    "\n",
    "def unique_combinations_baseline(directory):\n",
    "    unique_lists = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            info = extract_info_baseline(filename)\n",
    "            if info:\n",
    "                unique_lists.append(info)\n",
    "    \n",
    "    # Sort list based on initial_sample_size (int) first, Model (str) second\n",
    "    unique_lists = list(set(unique_lists)) # Remove duplicates\n",
    "    unique_lists.sort(key=lambda x: (x[2], x[0])) # Sort\n",
    "    \n",
    "    return unique_lists\n",
    "\n",
    "directories = ['Results/ID','Results/BO', 'Results/RF_ID', 'Results/RP_ID']\n",
    "\n",
    "# Get the unique combinations for each type of model\n",
    "unique_sets = []\n",
    "for directory in directories:\n",
    "    if directory == 'Results/ID':\n",
    "        unique_sets += unique_combinations(directory)\n",
    "    else:\n",
    "        unique_sets += unique_combinations_baseline(directory)\n",
    "\n",
    "# Generate the unique list\n",
    "unique_list = []\n",
    "for unique_set in unique_sets:\n",
    "    if \"gpt\" in unique_set[0]:  # For regular models\n",
    "        model, prompt, temp, target, _, prompt_chain  = unique_set\n",
    "        unique_list.append(f\"Model: {model}, Prompt: {prompt}, Temp: {temp}, Target: {target}, Prompt chain:{prompt_chain} \")\n",
    "    else:  # For baseline models\n",
    "        model, init_samples, target, budget = unique_set\n",
    "        unique_list.append(f\"Model: {model}, Initial Samples: {init_samples}, Target: {target}, Budget: {budget}\")\n",
    "\n",
    "combo_widget = widgets.SelectMultiple(\n",
    "    options=unique_list,\n",
    "    description='Combinations:',\n",
    "    layout=Layout(width='90%', height='350px')\n",
    ")\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(data, desired_target):\n",
    "    \n",
    "    with plot_output:\n",
    "        # Define the sorting key function\n",
    "        def sorting_key(config):\n",
    "            # Custom order for 'Information Quality'\n",
    "            info_quality_order = {'None': 0, 'Generic': 1, 'Specific': 2}\n",
    "            if len(config) == 5:  # For chat models with full configuration\n",
    "                info_quality = config[1]\n",
    "                return (0, info_quality_order.get(info_quality, -1)) + config  # 0 to prioritize chat models\n",
    "            else:  # For baseline methods, give a higher initial sort value\n",
    "                return (1, ) + config  # 1 to ensure baseline methods come after chat models\n",
    "\n",
    "        # Sort the data items\n",
    "        sorted_data = sorted(data.items(), key=lambda item: sorting_key(item[0]))\n",
    "\n",
    "        # Group data by model, strategy, and temperature, handling different config lengths\n",
    "        grouped_data = {}\n",
    "        for config, strengths in sorted_data:\n",
    "            if len(config) == 5:  # Configs with TT\n",
    "                group_key = tuple(config[:-1])  # Exclude the TT parameter\n",
    "                tt_index = int(config[-1])\n",
    "            else:  # Baseline methods\n",
    "                group_key = config\n",
    "                tt_index = 0  # Treat as TT=0\n",
    "\n",
    "            if group_key not in grouped_data:\n",
    "                grouped_data[group_key] = [None, None]  # Placeholder for TT=0 and TT=1\n",
    "            grouped_data[group_key][tt_index] = strengths\n",
    "\n",
    "        num_configs = len(grouped_data.keys())\n",
    "        ncols = 3\n",
    "        nrows = int(math.ceil(num_configs / ncols))\n",
    "        fig, axs = plt.subplots(nrows, ncols, figsize=(15, 5*nrows), sharex=True, sharey=True)\n",
    "        \n",
    "        # Ensure axs is always a 2D array\n",
    "        if nrows == 1 or ncols == 1:\n",
    "            axs = axs.reshape(nrows, ncols)\n",
    "\n",
    "        tt_colors = ['blue', 'pink']  # Blue for Baseline (TT=0), Orange for Increased TT (TT=1)\n",
    "\n",
    "        y_min = np.inf\n",
    "        y_max = -np.inf\n",
    "\n",
    "        for idx, (config, strengths_pair) in enumerate(grouped_data.items()):\n",
    "            row = idx // ncols\n",
    "            col = idx % ncols\n",
    "\n",
    "            for tt, all_strengths in enumerate(strengths_pair):\n",
    "                if all_strengths is None:\n",
    "                    continue  # Skip if no data for this TT value\n",
    "\n",
    "                # Calculate cumulative max for each experiment\n",
    "                cumulative_strengths = [np.maximum.accumulate(strength) for strength in all_strengths]\n",
    "\n",
    "                # Calculate the mean and the 10th and 90th percentiles\n",
    "                mean_strengths = np.mean(cumulative_strengths, axis=0)\n",
    "                lower_bound = np.percentile(cumulative_strengths, 10, axis=0)\n",
    "                upper_bound = np.percentile(cumulative_strengths, 90, axis=0)\n",
    "\n",
    "                iterations = list(range(1, len(mean_strengths) + 1))\n",
    "\n",
    "                # Plotting\n",
    "                label = 'Increased TT' if tt == 1 else 'Baseline'\n",
    "                axs[row, col].plot(iterations, mean_strengths, color=tt_colors[tt], label=label, linewidth=2)\n",
    "                axs[row, col].fill_between(iterations, lower_bound, upper_bound, alpha=0.3, color=tt_colors[tt])\n",
    "\n",
    "                # Update global y-axis limits\n",
    "                y_min = min(y_min, lower_bound.min())\n",
    "                y_max = max(y_max, upper_bound.max())\n",
    "\n",
    "            # Add labels, title, and legend for each subplot\n",
    "            axs[row, col].set_xlabel('Development Cycle')\n",
    "            axs[row, col].set_ylabel('Compressive Strength')\n",
    "            # Set title based on the length of config\n",
    "            if len(config) == 4:  # For chat models with full configuration\n",
    "                title = f\"Model: {config[0]}, Context Quality: {config[1]},\\nTemperature: {config[2]}\"\n",
    "            else:  # For baseline methods\n",
    "                title = f\"Baseline Method: {config[0]}\"\n",
    "\n",
    "            axs[row, col].set_title(title)\n",
    "\n",
    "            axs[row, col].grid(True)\n",
    "            axs[row, col].legend(loc='lower right')\n",
    "            # Add horizontal line for the desired target strength\n",
    "            axs[row, col].axhline(y=desired_target, color='r', linestyle='--')\n",
    "            axs[row, col].set_xlim(1, 10)\n",
    "\n",
    "        # Normalize y-axis for all subplots\n",
    "        for ax in axs.flat:\n",
    "            ax.set_ylim([y_min, y_max])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "# plot_results(data, desired_target)\n",
    "\n",
    "        \n",
    "import threading\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "# Create a lock\n",
    "lock = threading.Lock()\n",
    "\n",
    "def on_load_and_plot(btn):\n",
    "    # Acquire the lock\n",
    "    if not lock.acquire(blocking=False):\n",
    "        print('Another session is running, please wait...')\n",
    "        return\n",
    "\n",
    "    # Clear previous plots from the Output widget\n",
    "    plot_output.clear_output(wait=True)\n",
    "\n",
    "    data = load_selected_data(btn)  # store the returned data in a variable\n",
    "\n",
    "    # Draw the new plot inside the Output widget\n",
    "    with plot_output:\n",
    "        plot_results(data, desired_target= 64.86370000000001)\n",
    "\n",
    "    # Release the lock\n",
    "    lock.release()\n",
    "    \n",
    "# Define the button here\n",
    "load_button = widgets.Button(description='Load Data and Plot')\n",
    "\n",
    "load_button.on_click(on_load_and_plot)\n",
    "#display(combo_widget, load_button, plot_output)\n",
    "\n",
    "##########################\n",
    "# Add Table below:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def show_table(data):\n",
    "    def sorting_key(config):\n",
    "        info_quality_order = {'None': 0, 'Generic': 1, 'Specific': 2}\n",
    "        if len(config) == 5:  # For chat models with full configuration\n",
    "            return (0, info_quality_order.get(config[1], -1)) + config\n",
    "        else:  # For baseline methods\n",
    "            return (1, ) + config\n",
    "\n",
    "    def format_config_label(config):\n",
    "        if len(config) == 5:  # For chat models\n",
    "            return f\"{config[0]}, Context. Qual.: {config[1]}, Temp.: {config[2]}, Increased TT: {config[4]}\"\n",
    "        else:  # For baseline methods\n",
    "            return f\"Baseline Method: {config[0]}\"\n",
    "\n",
    "    def highlight_max(s):\n",
    "        is_max = s == s.max()\n",
    "        return ['font-weight: bold' if v else '' for v in is_max]\n",
    "\n",
    "    def highlight_second(s):\n",
    "        ordered = s.sort_values(ascending=False)\n",
    "        if len(ordered) > 1:\n",
    "            is_second = s == ordered.iloc[1]\n",
    "        else:\n",
    "            is_second = [False] * len(s)\n",
    "        return ['text-decoration: underline' if v else '' for v in is_second]\n",
    "\n",
    "    def highlight_third(s):\n",
    "        ordered = s.sort_values(ascending=False)\n",
    "        if len(ordered) > 2:\n",
    "            is_third = s == ordered.iloc[2]\n",
    "        else:\n",
    "            is_third = [False] * len(s)\n",
    "        return ['font-style: italic' if v else '' for v in is_third]\n",
    "\n",
    "    sorted_data = sorted(data.items(), key=lambda item: sorting_key(item[0]))\n",
    "    mean_dict = {'1st': [],'2nd': [],'3rd': [],'4th': [], '5th': [],'6th': [],'7th': [],'8th': [],'9th': [], '10th': []}\n",
    "    lower_bound_dict =  {'1st': [],'2nd': [],'3rd': [],'4th': [], '5th': [],'6th': [],'7th': [],'8th': [],'9th': [], '10th': []}\n",
    "    success_rate_dict =  {'1st': [],'2nd': [],'3rd': [],'4th': [], '5th': [],'6th': [],'7th': [],'8th': [],'9th': [], '10th': []}\n",
    "    configs = []\n",
    "    threshold = 64.86370000000001\n",
    "\n",
    "    for config, all_strengths in sorted_data:\n",
    "        label = format_config_label(config)\n",
    "        \n",
    "        configs.append(label)\n",
    "        all_strengths = [np.maximum.accumulate(strength) for strength in all_strengths]\n",
    "        mean_strengths = np.mean(all_strengths, axis=0)\n",
    "        lower_bound = np.percentile(all_strengths, 10, axis=0)\n",
    "        cumulative_success = np.cumsum([strength > threshold for strength in all_strengths], axis=1)\n",
    "        success_rates = (cumulative_success / (np.arange(len(all_strengths[0])) + 1)) * 100  # calculate cumulative success rates\n",
    "        indices = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            mean_value = mean_strengths[idx] if idx < len(mean_strengths) else np.nan\n",
    "            lower_bound_value = lower_bound[idx] if idx < len(lower_bound) else np.nan\n",
    "            success_rate_value = success_rates[:, idx].mean() if idx < len(mean_strengths) else np.nan\n",
    "            mean_dict[list(mean_dict.keys())[i]].append(mean_value)\n",
    "            lower_bound_dict[list(lower_bound_dict.keys())[i]].append(lower_bound_value)\n",
    "            success_rate_dict[list(success_rate_dict.keys())[i]].append(success_rate_value)\n",
    "    \n",
    "    df_mean = pd.DataFrame(mean_dict, index=configs).applymap(lambda x: round(x, 2))\n",
    "    df_lower_bound = pd.DataFrame(lower_bound_dict, index=configs).applymap(lambda x: round(x, 2))\n",
    "    df_success_rate = pd.DataFrame(success_rate_dict, index=configs).applymap(lambda x: round(x, 2))\n",
    "\n",
    "    print(\"Mean values:\")\n",
    "    display(df_mean.style.format(\"{:.2f}\").apply(highlight_max).apply(highlight_second).apply(highlight_third))\n",
    "    print(\"Lower bound values:\")\n",
    "    display(df_lower_bound.style.format(\"{:.2f}\").apply(highlight_max).apply(highlight_second).apply(highlight_third))\n",
    "    print(\"Cumulative Success Rate:\")\n",
    "    display(df_success_rate.style.format(\"{:.2f}\").apply(highlight_max).apply(highlight_second).apply(highlight_third))\n",
    "\n",
    "    df_mean.to_csv(os.path.join('Results/', 'mean_values.csv'), sep=';', decimal=',', index=True)\n",
    "    df_lower_bound.to_csv(os.path.join('Results/', 'lower_bound_values.csv'), sep=';', decimal=',', index=True)\n",
    "    df_success_rate.to_csv(os.path.join('Results/', 'cumulative_success_rate.csv'), sep=';', decimal=',', index=True)\n",
    "\n",
    "# Create a button for showing the table\n",
    "show_button = widgets.Button(description='Show Table')\n",
    "\n",
    "def on_show_table(btn):\n",
    "    # Acquire the lock\n",
    "    if not lock.acquire(blocking=False):\n",
    "        print('Another session is running, please wait...')\n",
    "        return\n",
    "\n",
    "    # Clear previous tables from the Output widget\n",
    "    table_output.clear_output(wait=True)\n",
    "\n",
    "    data = load_selected_data(btn)  # we assume that this function loads the selected data\n",
    "\n",
    "    # Draw the new table inside the Output widget\n",
    "    with table_output:\n",
    "        show_table(data)\n",
    "\n",
    "    # Release the lock\n",
    "    lock.release()\n",
    "\n",
    "show_button.on_click(on_show_table)\n",
    "\n",
    "table_output = widgets.Output()\n",
    "\n",
    "display(combo_widget, load_button, plot_output, show_button, table_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657039b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
